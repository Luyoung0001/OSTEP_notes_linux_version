## 多处理器调度

缓存一致性问题的解决

硬件提供了这个问题的基本解决方案：通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式是使用总线窥探（bus snooping）[G83]。每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果CPU发现对它放在缓存中的数据的更新，会作废（invalidate）本地副本（从缓存中移除），或更新（update）它（修改为新值）。回写缓存，如上面提到的，让事情更复杂（由于对内存的写入稍后才会看到），你可以想想基本方案如何工作。

## 别忘了同步

尽管操作系统帮我们做了很多工作来实现数据同步,但是我们依然在进行多 CPU 调度的时候,需要采用一些特定语句,来实现数据共享.

跨CPU访问（尤其是写入）共享数据或数据结构时，需要使用互斥原语（比如锁），才能保证正确性（其他方法，如使用无锁（lock-free）数据结构，很复杂，偶尔才使用。详情参见并发部分关于死锁的章节）。例如，假设多CPU并发访问一个共享队列。如果没有锁，即使有底层一致性协议，并发地从队列增加或删除元素，依然不会得到预期结果。需要用锁来保证数据结构状态更新的原子性。

好消息是,系统为我们提供了一些 API,这些 API 能满足多CPU 调度并解决数据访问一致性问题.

## 最后一个问题: 缓存亲合度

在 cpuA 上运行某个进程时,在 CPU 的缓存中会维护很多关于这个进程的状态.但是在不同的 CPU上运行相同的进程时,会重新将这些缓存加载cpu 的缓存中,这就很慢了.

因此,多处理器调度应该考虑到这种缓存新合兴,并尽可能将进程保持在同一个 CPU 上.

## 单队列调度

上面介绍了一些背景，现在来讨论如何设计一个多处理器系统的调度程序。最基本的方式是简单地复用单处理器调度的基本架构，将所有需要调度的工作放入一个单独的队列中，我们称之为单队列多处理器调度（Single Queue Multiprocessor Scheduling，SQMS）

然而,为了保证在多 CPU 上正常运行,调度程序的开发者需要在代码中通过加锁来保证原子性.

但是锁可能带来巨大的性能损失,尤其是随着系统中的 CPU 数量增加时.

另一个问题是缓存亲和性被破坏.为了解决这个问题,大多数 SQMS调度程序都引入了一些亲合度机制,尽可能让进程在同一个 CPU 上运行.

我们看到，SQMS调度方式有优势也有不足。优势是能够从单CPU调度程序很简单地发展而来，根据定义，它只有一个队列。然而，它的扩展性不好（由于同步开销有限），并且不能很好地保证缓存亲和度。

## 多队列调度

正是由于单队列调度程序的这些问题，有些系统使用了多队列的方案，比如每个CPU一个队列。我们称之为多队列多处理器调度（Multi-Queue Multiprocessor Scheduling，MQMS）

在MQMS中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则，比如轮转或其他任何可能的算法。当一个工作进入系统后，系统会依照一些启发性规则（如随机或选择较空的队列）将其放入某个调度队列。

$这样一来，每个CPU调度之间相互独立，就避免了单队列的方式中由于数据共享及同步带来的问题$。

## MQMS 的问题
A获得了B和D两倍的CPU时间，这不是期望的结果。更糟的是，假设A和C都执行完毕，系统中只有B和D。

怎样才能克服潜伏的负载不均问题，打败邪恶的……霸天虎军团[1]？如何才能不要问这些与这本好书几乎无关的问题？

最明显的答案是让工作移动，这种技术我们称为迁移（migration）。通过工作的跨CPU迁移，可以真正实现负载均衡。

这就解决了,8 核 CPU,7 核围观一个核工作的可笑场景.

## 但现在是最棘手的部分：系统如何决定发起这样的迁移？

一个基本的方法是采用一种技术，名为工作窃取（work stealing）[FLR98]。通过这种方法，工作量较少的（源）队列不定期地“偷看”其他（目标）队列是不是比自己的工作多。如果目标队列比源队列（显著地）更满，就从目标队列“窃取”一个或多个工作，实现负载均衡。当然，这种方法也有让人抓狂的地方——如果太频繁地检查其他队列，就会带来较高的开销，可扩展性不好，而这是多队列调度最初的全部目标！相反，如果检查间隔太长，又可能会带来严重的负载不均。找到合适的阈值仍然是黑魔法，这在系统策略设计中很常见。

## 总结
本章介绍了多处理器调度的不同方法。其中单队列的方式（SQMS）比较容易构建，负载均衡较好，但在扩展性和缓存亲和度方面有着固有的缺陷。多队列的方式（MQMS）有很好的扩展性和缓存亲和度，但实现负载均衡却很困难，也更复杂。无论采用哪种方式，都没有简单的答案：构建一个通用的调度程序仍是一项令人生畏的任务，因为即使很小的代码变动，也有可能导致巨大的行为差异。除非很清楚自己在做什么，或者有人付你很多钱，否则别干这种事。

## BFS
BFS全称：Brain Fu*k Scheduler，脑残调度器

因此，考虑到用一种调度算法应对一种场景的思想，Con Kolivas提出了BFS。

BFS面向桌面Linux，适用与较少的CPU，BFS算法简单，因此从根本上降低了开销

在一些低端设备，例如在2CPU、4CPU的设备上，BFS的表现要比CFS明显好很多.

BFS是如何运行的：

首先，BFS会使用bitmap trick来查看对应优先级的队列是不是空的，也就是该优先级下的任务是否存在。

如果存在，BFS会遍历整个队列，找到virtual deadline最小的任务，也就是让CPU先去做那些能最早完成的任务。 可以简单粗暴的理解为，BFS让系统倾向于去做那些较为简单的任务，而把耗时、棘手的任务放在后面。

当任务耗尽时间片之后：

任务从CPU上撤下，

新任务重新占据CPU，开始一个新的timeslice

重新计算virtual deadline

当一个任务休眠后，它的timeslice和deadline不变

新唤醒的任务比当前正在运行的任务具有更高的优先级或更早的截止日期：

好了，以上就是BFS的运行流程，相信看到这你可能会奇怪，virtual deadline是什么？为什么要有这个东西？

virtual deadline正如其名，它是一个虚拟的ddl，并不是这个任务的真正结束时间（可以把它想象成任务排期）。我们用这个参数的目的，是想要评估任务的大概结束时间。假如一个任务的deadline比较晚，那就意味着这个任务比较麻烦、耗时，我们就把它放到后面去做。

值得注意的是，virtual deadline与nice值紧密相关，由公式可以得出，那些高优先级的任务，也就是nice较小的任务，他们的vd相对较小，因此会有更多得到时间片的机会。

但是，根据第三条，新唤醒的任务会具有抢占性，会抢占当前CPU正在运行的任务，这是BFS具有交互性的由来。